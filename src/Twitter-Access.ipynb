{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -X GET -H \"Authorization: Bearer <BEARER TOKEN>\" \"https://api.twitter.com/2/tweets/20\"\n",
    "# bash command to hydrate an example tweet\n",
    "# get your bearer token (and the keys) here -> https://developer.twitter.com/en/portal/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Enter your keys/secrets as strings in the following fields\n",
    "credentials = {}\n",
    "credentials['CONSUMER_KEY'] = \"xxx\"  # key names not consistent ->  API KEY = CONSUMER_KEY\n",
    "credentials['CONSUMER_SECRET'] = \"xxx\" # API SECRET KEY = CONSUMER_SECRET\n",
    "credentials['ACCESS_TOKEN'] = \"xxx\"   # not needed for now?\n",
    "credentials['ACCESS_SECRET'] = \"xxx\" # not needed for now?\n",
    "credentials['BEARER_TOKEN'] = 'xxx' # seee celll above\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install twython\n",
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twython class\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "# Load credentials from json file\n",
    "\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(credentials['CONSUMER_KEY'], credentials['CONSUMER_SECRET'],credentials['ACCESS_TOKEN'],credentials['ACCESS_SECRET'],credentials['BEARER_TOKEN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show json for a specific ID\n",
    "from pprint import pprint\n",
    "tweet= python_tweets.show_status(id=\"1245219745311404032\")\n",
    "#pprint(tweet)\n",
    "\n",
    "# path to location. but path varies, so it may fail for other IDS\n",
    "# -> it fails for every tweet that is not this tweet (see evaluation in next ceel)\n",
    "# -> TODO better generalization / method\n",
    "#pprint (tweet[\"extended_entities\"][\"media\"][0][\"additional_media_info\"][\"source_user\"]['location'])\n",
    "#print(tweet[\"place\"][\"country\"])\n",
    "#print(tweet[\"created_at\"].split()[1])\n",
    "#print(tweet[\"created_at\"].split()[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "\n",
    "# request limit: 900 requestis/15 mins.\n",
    "# so if you make 1 request per second, everything will be fine. (15*60 secs = 900secs)\n",
    "# but it seems to be a rolling window, so you don't have to wait for a full reset, when limit is reached. \n",
    "# adjust sleeping time as required. \n",
    "# DONE dynamische Berechnung der time-limits um ensprechend anfragefrequenz automatisch regeln\n",
    "\n",
    "import time\n",
    "\n",
    "def calc_delay_to_not_run_into_rate_limit():\n",
    "    # good if you make requests over long time (>15 mins). if not, shorter delay times then returned here would be ok\n",
    "    # calculation based on the 'python_tweets.show_status(id)'-method. if we use a batch-request-method other time limits apply.\n",
    "    python_tweets.get_application_rate_limit_status(resources=\"statuses\")\n",
    "    resource = python_tweets.get_application_rate_limit_status() # if you get a permession denied error here, you have to change your \"project settings\" here (developer.twitter.com), to \"Read, Write, and Direct Messages\" and regenerate all keys. TODO add to readme\n",
    "    resource = resource[\"resources\"][\"statuses\"][\"/statuses/show/:id\"] \n",
    "    # -> {'limit': 900, 'remaining': 737, 'reset': 1607445919}\n",
    "    should_sleep = (resource[\"reset\"] - time.time()) / resource[\"remaining\"]\n",
    "    return should_sleep # seconds to sleep before next request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 1/533 [00:01<11:00,  1.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1613133d9884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mdelay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_delay_to_not_run_into_rate_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "count = 0\n",
    "directory = \"data/GeoCOV19TweetsDataset\"\n",
    "\n",
    "# For each file/day\n",
    "for entry in os.scandir(directory):\n",
    "    count+=1\n",
    "    \n",
    "    # Load csv file containing the tweet ID's\n",
    "    tweets_csv =  pd.read_csv(entry.path)\n",
    "    tweets_csv.columns = [\"ID\",\"Sentiment\"] \n",
    "    del tweets_csv[\"Sentiment\"] # del sentiment column\n",
    "\n",
    "    # Inialize panda dataframe for the tweets\n",
    "    # There will probably be one DF for each day\n",
    "    Twitter_Tweets = pd.DataFrame(\n",
    "    columns=['ID', 'COUNTRY', 'DAY', 'MONTH', 'TEXT_RAW'])\n",
    "    # Only countries from the following list are considered \n",
    "    vonInteresse=[\"Vereinigte Staaten\", \"Republik Indien\", \"Vereinigtes Königreich\"]\n",
    "\n",
    "    # For each tweet in the file\n",
    "    tweet_not_found=0\n",
    "    for i in tqdm(range(len(tweets_csv))):\n",
    "        # Delay the tweet extraction since otherwise the rate limit exceeds \n",
    "        if i%50 == 0:\n",
    "            delay = calc_delay_to_not_run_into_rate_limit()\n",
    "        time.sleep(delay)           \n",
    "\n",
    "        ID = int(tweets_csv['ID'].values[i])\n",
    "        try:\n",
    "            tweet= python_tweets.show_status(id=ID)\n",
    "        except Exception as e: \n",
    "            tweet_not_found += 1\n",
    "            continue\n",
    "\n",
    "        # If all values exist add the tweets ID, Country, Day, Month and raw text to the DF\n",
    "        try:\n",
    "            if str(tweet[\"place\"][\"country\"]) in vonInteresse:\n",
    "                data = {'ID': ID, \n",
    "                'COUNTRY': tweet[\"place\"][\"country\"], \n",
    "                'DAY': tweet[\"created_at\"].split()[2],\n",
    "                'MONTH': tweet[\"created_at\"].split()[1],\n",
    "                'TEXT_RAW': tweet[\"text\"]} \n",
    "                Twitter_Tweets = Twitter_Tweets.append(data, ignore_index=True)\n",
    "                print(tweet[\"text\"])\n",
    "                print(\"\")\n",
    "        except: \n",
    "            continue\n",
    "\n",
    "    # Save DF to csv file\n",
    "    Twitter_Tweets.to_csv (\"data/Tweets/\" + os.path.basename(entry.path), index = False, header=True)\n",
    "    print(\"\")\n",
    "    print(\"File \" + str(count) + \" contains: \" + str(len(Twitter_Tweets)) + \" Files\") \n",
    "    print(\"\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
